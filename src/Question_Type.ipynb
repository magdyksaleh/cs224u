{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict question type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'number': 5850, 'spans': 3529, 'date': 157})\n",
      "Counter({'arithmetic': 3542, 'spans': 3529, 'count': 2308, 'date': 157})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc3e4544a70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Iterator, List, Dict\n",
    "import torch\n",
    "from util import ArrayField\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, SequenceLabelField, LabelField\n",
    "\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.dataset_readers.reading_comprehension.drop import DropReader\n",
    "from allennlp.data.tokenizers import Token, WordTokenizer\n",
    "\n",
    "from allennlp.data.dataset_readers.reading_comprehension.util import (IGNORED_TOKENS,\n",
    "                                                                      STRIPPED_CHARACTERS,\n",
    "                                                                      make_reading_comprehension_instance,\n",
    "                                                                      split_tokens_by_hyphen)\n",
    "\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper \n",
    "from allennlp.modules.seq2vec_encoders import PytorchSeq2VecWrapper, Seq2VecEncoder\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from allennlp.training.metrics import BooleanAccuracy, CategoricalAccuracy\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.predictors import SentenceTaggerPredictor\n",
    "\n",
    "from allennlp.common import JsonDict\n",
    "from allennlp.data import DatasetReader, Instance\n",
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from allennlp.models import Model\n",
    "from allennlp.predictors import Predictor\n",
    "from overrides import overrides\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_NUMBER_MAP = {\"zero\": 0, \"one\": 1, \"two\": 2, \"three\": 3, \"four\": 4,\n",
    "                   \"five\": 5, \"six\": 6, \"seven\": 7, \"eight\": 8,\n",
    "                   \"nine\": 9, \"ten\": 10, \"eleven\": 11, \"twelve\": 12,\n",
    "                   \"thirteen\": 13, \"fourteen\": 14, \"fifteen\": 15,\n",
    "                   \"sixteen\": 16, \"seventeen\": 17, \"eighteen\": 18, \"nineteen\": 19}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropTypeDatasetReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    DatasetReader\n",
    "    \"\"\"\n",
    "    def __init__(self, token_indexers: Dict[str, TokenIndexer] = None) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self._tokenizer = WordTokenizer()\n",
    "        \n",
    "    def text_to_instance(self, passage_numbers: List[int], tokens_q: List[Token], tag: List[str] = None) -> Instance:\n",
    "        question_field = TextField(tokens_q, self.token_indexers) \n",
    "        number_field = ArrayField(np.array(passage_numbers))\n",
    "        fields = {\"question\": question_field, \"numbers_in_passage\": number_field}\n",
    "\n",
    "        if tag:\n",
    "            label_field = LabelField(tag)\n",
    "            fields[\"label\"] = label_field\n",
    "        return Instance(fields)\n",
    "    \n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        with open(file_path) as json_file:  \n",
    "            data = json.load(json_file)\n",
    "            for k in data:\n",
    "                passage = data[k]['passage']\n",
    "                passage_tokens =  split_tokens_by_hyphen(self._tokenizer.tokenize(passage))\n",
    "                passage_numbers = self.get_numbers_in_passage(passage_tokens)\n",
    "                for elem in data[k]['qa_pairs']:\n",
    "                    question = elem['question']\n",
    "                    answer_type = \"\" \n",
    "                    for key in elem['answer']:\n",
    "                        if (key == 'number') and (len(elem['answer'][key]) != 0):\n",
    "                            if float(elem['answer']['number']) > 9:\n",
    "                                answer_type = \"arithmetic\"\n",
    "                            else: \n",
    "                                answer_type = \"count\"\n",
    "                            break\n",
    "\n",
    "                        if (key == 'spans') and (len(elem['answer'][key]) != 0):\n",
    "                            if (elem['answer'][key][0] in question):\n",
    "                                answer_type = \"span_in_question\"\n",
    "                            else: \n",
    "                                answer_type = \"span_in_passage\"\n",
    "                            break\n",
    "\n",
    "                        answer_type = 'date'\n",
    "                    yield self.text_to_instance(passage_numbers, [Token(word) for word in question], answer_type)\n",
    "\n",
    "    \n",
    "    def get_numbers_in_passage(self, passage_tokens):\n",
    "        \"\"\"\n",
    "        Returns list of numbers in the passage\n",
    "        \"\"\"\n",
    "        numbers_in_passage = []\n",
    "        number_indices = []\n",
    "        for token_index, token in enumerate(passage_tokens):\n",
    "            number = self.convert_word_to_number(token.text)\n",
    "            if number is not None:\n",
    "                numbers_in_passage.append(number)\n",
    "                number_indices.append(token_index)\n",
    "\n",
    "        return numbers_in_passage\n",
    "                    \n",
    "    def convert_word_to_number(self, word: str, try_to_include_more_numbers=False):\n",
    "        \"\"\"\n",
    "        Currently we only support limited types of conversion.\n",
    "        \"\"\"\n",
    "        if try_to_include_more_numbers:\n",
    "            # strip all punctuations from the sides of the word, except for the negative sign\n",
    "            punctruations = string.punctuation.replace('-', '')\n",
    "            word = word.strip(punctruations)\n",
    "            # some words may contain the comma as deliminator\n",
    "            word = word.replace(\",\", \"\")\n",
    "            # word2num will convert hundred, thousand ... to number, but we skip it.\n",
    "            if word in [\"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]:\n",
    "                return None\n",
    "            try:\n",
    "                number = word_to_num(word)\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    number = int(word)\n",
    "                except ValueError:\n",
    "                    try:\n",
    "                        number = float(word)\n",
    "                    except ValueError:\n",
    "                        number = None\n",
    "            return number\n",
    "        else:\n",
    "            no_comma_word = word.replace(\",\", \"\")\n",
    "            if no_comma_word in WORD_NUMBER_MAP:\n",
    "                number = WORD_NUMBER_MAP[no_comma_word]\n",
    "            else:\n",
    "                try:\n",
    "                    number = int(no_comma_word)\n",
    "                except ValueError:\n",
    "                    number = None\n",
    "            return number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = DropTypeDatasetReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77409it [00:34, 2275.55it/s]\n",
      "9536it [00:04, 2243.99it/s]\n"
     ]
    }
   ],
   "source": [
    "data_train = reader.read('../data/drop_dataset/drop_dataset_train.json')\n",
    "data_dev = reader.read('../data/drop_dataset/drop_dataset_dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86945/86945 [00:02<00:00, 32190.10it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(data_train + data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model in AllenNLP represents a model that is trained.\n",
    "class LstmClassifier(Model):\n",
    "    def __init__(self,\n",
    "                 word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder,\n",
    "                 vocab: Vocabulary,\n",
    "                 num_pad_size=10) -> None:\n",
    "        super().__init__(vocab)\n",
    "        # We need the embeddings to convert word IDs to their vector representations\n",
    "        self.word_embeddings = word_embeddings\n",
    "\n",
    "        # Seq2VecEncoder is a neural network abstraction that takes a sequence of something\n",
    "        # (usually a sequence of embedded word vectors), processes it, and returns it as a single\n",
    "        # vector. Oftentimes, this is an RNN-based architecture (e.g., LSTM or GRU), but\n",
    "        # AllenNLP also supports CNNs and other simple architectures (for example,\n",
    "        # just averaging over the input vectors).\n",
    "        self.encoder = encoder\n",
    "        self.num_pad_size = num_pad_size\n",
    "        # After converting a sequence of vectors to a single vector, we feed it into\n",
    "        # a fully-connected linear layer to reduce the dimension to the total number of labels.\n",
    "        self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim()+self.num_pad_size,\n",
    "                                          out_features=vocab.get_vocab_size('labels'))\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "\n",
    "        # We use the cross-entropy loss because this is a classification task.\n",
    "        # Note that PyTorch's CrossEntropyLoss combines softmax and log likelihood loss,\n",
    "        # which makes it unnecessary to add a separate softmax layer.\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Instances are fed to forward after batching.\n",
    "    # Fields are passed through arguments with the same name.\n",
    "    def forward(self,\n",
    "                question: Dict[str, torch.Tensor],\n",
    "                numbers_in_passage: List[int],\n",
    "                label: torch.Tensor = None) -> torch.Tensor:\n",
    "        # In deep NLP, when sequences of tensors in different lengths are batched together,\n",
    "        # shorter sequences get padded with zeros to make them of equal length.\n",
    "        # Masking is the process to ignore extra zeros added by padding\n",
    "        mask = get_text_field_mask(question)\n",
    "\n",
    "        # Forward pass\n",
    "        embeddings = self.word_embeddings(question)\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "#         print(encoder_out.shape)\n",
    "#         print(numbers_in_passage.shape)\n",
    "        inputs = torch.cat((encoder_out, numbers_in_passage), 1)\n",
    "        logits = self.hidden2tag(inputs)\n",
    "\n",
    "        # In AllenNLP, the output of forward() is a dictionary.\n",
    "        # Your output dictionary must contain a \"loss\" key for your model to be trained.\n",
    "        output = {\"logits\": logits}\n",
    "        if label is not None:\n",
    "            self.accuracy(logits, label)\n",
    "            output[\"loss\"] = self.loss_function(logits, label)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 10\n",
    "HIDDEN_DIM = 10\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = PytorchSeq2VecWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
    "model = LstmClassifier(word_embeddings, lstm, vocab, num_pad_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    cuda_device = 0\n",
    "    model = model.cuda(cuda_device)\n",
    "else:\n",
    "    cuda_device = -1\n",
    "    \n",
    "cuda_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.3490, loss: 16649997555.0748 ||: 100%|██████████| 1210/1210 [00:32<00:00, 37.26it/s]\n",
      "accuracy: 0.3531, loss: 6415.6026 ||: 100%|██████████| 149/149 [00:02<00:00, 54.58it/s]\n",
      "accuracy: 0.3547, loss: 8841083838.0105 ||: 100%|██████████| 1210/1210 [00:27<00:00, 43.88it/s]\n",
      "accuracy: 0.3619, loss: 4436.4880 ||: 100%|██████████| 149/149 [00:02<00:00, 71.37it/s]\n",
      "accuracy: 0.3596, loss: 1521857957.9137 ||: 100%|██████████| 1210/1210 [00:27<00:00, 44.46it/s]\n",
      "accuracy: 0.3788, loss: 2392.7756 ||: 100%|██████████| 149/149 [00:02<00:00, 73.31it/s]\n",
      "accuracy: 0.4634, loss: 796968797.0598 ||: 100%|██████████| 1210/1210 [00:29<00:00, 41.40it/s]\n",
      "accuracy: 0.4691, loss: 1175.0075 ||: 100%|██████████| 149/149 [00:02<00:00, 71.66it/s]\n",
      "accuracy: 0.5490, loss: 1185537093.8536 ||: 100%|██████████| 1210/1210 [00:27<00:00, 44.45it/s]\n",
      "accuracy: 0.3811, loss: 3124.6249 ||: 100%|██████████| 149/149 [00:02<00:00, 72.05it/s]\n",
      "accuracy: 0.5378, loss: 1512287138.1313 ||: 100%|██████████| 1210/1210 [00:27<00:00, 44.77it/s]\n",
      "accuracy: 0.4611, loss: 3234.9391 ||: 100%|██████████| 149/149 [00:02<00:00, 73.26it/s]\n",
      "accuracy: 0.5790, loss: 1506870710.9981 ||: 100%|██████████| 1210/1210 [00:26<00:00, 51.43it/s]\n",
      "accuracy: 0.5911, loss: 917.4714 ||: 100%|██████████| 149/149 [00:02<00:00, 71.99it/s]\n",
      "accuracy: 0.5798, loss: 1347888341.1887 ||: 100%|██████████| 1210/1210 [00:27<00:00, 48.41it/s]\n",
      "accuracy: 0.5581, loss: 1405.2335 ||: 100%|██████████| 149/149 [00:02<00:00, 71.46it/s]\n",
      "accuracy: 0.5809, loss: 788473230.0305 ||: 100%|██████████| 1210/1210 [00:27<00:00, 44.28it/s]\n",
      "accuracy: 0.6022, loss: 1118.4615 ||: 100%|██████████| 149/149 [00:02<00:00, 72.71it/s]\n",
      "accuracy: 0.6009, loss: 1237878243.3075 ||: 100%|██████████| 1210/1210 [00:27<00:00, 44.43it/s]\n",
      "accuracy: 0.5616, loss: 2802.2577 ||: 100%|██████████| 149/149 [00:02<00:00, 71.68it/s]\n",
      "accuracy: 0.6010, loss: 815069138.8495 ||: 100%|██████████| 1210/1210 [00:26<00:00, 44.92it/s]\n",
      "accuracy: 0.5548, loss: 2688.9865 ||: 100%|██████████| 149/149 [00:02<00:00, 72.38it/s]\n",
      "accuracy: 0.6187, loss: 1451656633.2476 ||: 100%|██████████| 1210/1210 [00:27<00:00, 44.51it/s]\n",
      "accuracy: 0.5955, loss: 2337.2397 ||: 100%|██████████| 149/149 [00:02<00:00, 72.40it/s]\n",
      "accuracy: 0.6217, loss: 1241965114.7272 ||: 100%|██████████| 1210/1210 [00:27<00:00, 44.27it/s]\n",
      "accuracy: 0.5633, loss: 2009.9427 ||: 100%|██████████| 149/149 [00:02<00:00, 72.79it/s]\n",
      "accuracy: 0.6219, loss: 1080556665.6802 ||: 100%|██████████| 1210/1210 [00:29<00:00, 41.58it/s]\n",
      "accuracy: 0.5084, loss: 3682.2192 ||: 100%|██████████| 149/149 [00:02<00:00, 72.91it/s]\n",
      "accuracy: 0.5969, loss: 7785483.3116 ||:  69%|██████▊   | 830/1210 [00:19<00:07, 47.73it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ac29f70ddf92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                   \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                   cuda_device=cuda_device)\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/allennlp/training/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_counter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m             \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;31m# get peak of memory usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/allennlp/training/trainer.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mbatch_grad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrescale_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
=======
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.2890, loss: 2202755643.9491 ||: 100%|██████████| 1210/1210 [00:49<00:00, 24.49it/s]\n",
      "accuracy: 0.3246, loss: 9017.2825 ||: 100%|██████████| 149/149 [00:02<00:00, 56.21it/s]\n",
      "accuracy: 0.3601, loss: 1224447774.9484 ||: 100%|██████████| 1210/1210 [00:47<00:00, 25.62it/s]\n",
      "accuracy: 0.3427, loss: 6584.2531 ||: 100%|██████████| 149/149 [00:02<00:00, 50.37it/s]\n",
      "accuracy: 0.3104, loss: 776972266.3957 ||: 100%|██████████| 1210/1210 [00:45<00:00, 25.06it/s]\n",
      "accuracy: 0.3026, loss: 5683.9127 ||: 100%|██████████| 149/149 [00:02<00:00, 52.53it/s]\n",
      "accuracy: 0.2618, loss: 2098469471.6363 ||: 100%|██████████| 1210/1210 [00:45<00:00, 26.84it/s]\n",
      "accuracy: 0.2734, loss: 4754.3521 ||: 100%|██████████| 149/149 [00:03<00:00, 44.50it/s]\n",
      "accuracy: 0.2712, loss: 950401178.8272 ||: 100%|██████████| 1210/1210 [00:44<00:00, 27.03it/s]\n",
      "accuracy: 0.2428, loss: 4222.9842 ||: 100%|██████████| 149/149 [00:02<00:00, 71.84it/s]\n",
      "accuracy: 0.2892, loss: 1433437176.8942 ||: 100%|██████████| 1210/1210 [00:43<00:00, 27.53it/s]\n",
      "accuracy: 0.2927, loss: 2814.6758 ||: 100%|██████████| 149/149 [00:02<00:00, 73.76it/s]\n",
      "accuracy: 0.3067, loss: 1304204718.2511 ||: 100%|██████████| 1210/1210 [00:44<00:00, 27.11it/s]\n",
      "accuracy: 0.2253, loss: 3486.1289 ||: 100%|██████████| 149/149 [00:02<00:00, 71.41it/s]\n",
      "accuracy: 0.2897, loss: 52841.8683 ||:  26%|██▌       | 314/1210 [00:12<00:29, 30.60it/s]"
>>>>>>> 76f01f1d223b39d8e6286821b5866fb3f4a2dec5
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "iterator = BucketIterator(batch_size=128, sorting_keys=[(\"question\", \"num_tokens\")])\n",
    "iterator.index_with(vocab)\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=data_train,\n",
    "                  validation_dataset=data_dev,\n",
    "                  patience=10,\n",
<<<<<<< HEAD
    "                  num_epochs=40,\n",
    "                  cuda_device=cuda_device)\n",
=======
    "                  num_epochs=10,\n",
    "                  cuda_device=-1)\n",
>>>>>>> 76f01f1d223b39d8e6286821b5866fb3f4a2dec5
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to name your predictor and register so that `allennlp` command can recognize it\n",
    "# Note that you need to use \"@Predictor.register\", not \"@Model.register\"!\n",
    "@Predictor.register(\"sentence_classifier_predictor2\")\n",
    "class SentenceClassifierPredictor(Predictor):\n",
    "    def __init__(self, model: Model, dataset_reader: DatasetReader) -> None:\n",
    "        super().__init__(model, dataset_reader)\n",
    "        self._tokenizer = SpacyWordSplitter(language='en_core_web_sm', pos_tags=True)\n",
    "\n",
    "    def predict(self, sentence: str) -> JsonDict:\n",
    "        return self.predict_json({\"question\" : question})\n",
    "\n",
    "    @overrides\n",
    "    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n",
    "        sentence = json_dict[\"question\"]\n",
    "        tokens = self._tokenizer.split_words(question)\n",
    "        return self._dataset_reader.text_to_instance([str(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument 'string' has incorrect type (expected str, got list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c98a595f5e1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'This'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'the'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'best'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'movie'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ever'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'!'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceClassifierPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_reader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlabel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-00fb9c34fba8>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"sentence\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Stanford/winter2019/pyEnv/lib/python3.7/site-packages/allennlp/predictors/predictor.py\u001b[0m in \u001b[0;36mpredict_json\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json_to_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-00fb9c34fba8>\u001b[0m in \u001b[0;36m_json_to_instance\u001b[0;34m(self, json_dict)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_json_to_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mInstance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_to_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Stanford/winter2019/pyEnv/lib/python3.7/site-packages/allennlp/data/tokenizers/word_splitter.py\u001b[0m in \u001b[0;36msplit_words\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msplit_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mToken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;31m# This works because our Token class matches spacy's.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_remove_spaces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Stanford/winter2019/pyEnv/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             )\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Stanford/winter2019/pyEnv/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument 'string' has incorrect type (expected str, got list)"
     ]
    }
   ],
   "source": [
    "tokens = ['This', 'is', 'the', 'best', 'movie', 'ever', '!']\n",
    "predictor = SentenceClassifierPredictor(model, dataset_reader=reader)\n",
    "logits = predictor.predict(tokens)['logits']\n",
    "label_id = np.argmax(logits)\n",
    "\n",
    "print(model.vocab.get_token_from_index(label_id, 'labels'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
