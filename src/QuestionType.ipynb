{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict question type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x117b51eb0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Iterator, List, Dict\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import json\n",
    "import numpy as np\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, SequenceLabelField, LabelField\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper \n",
    "from allennlp.modules.seq2vec_encoders import PytorchSeq2VecWrapper, Seq2VecEncoder\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from allennlp.training.metrics import BooleanAccuracy, CategoricalAccuracy\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.predictors import SentenceTaggerPredictor\n",
    "\n",
    "from allennlp.common import JsonDict\n",
    "from allennlp.data import DatasetReader, Instance\n",
    "from allennlp.data.tokenizers.word_splitter import SpacyWordSplitter\n",
    "from allennlp.models import Model\n",
    "from allennlp.predictors import Predictor\n",
    "from overrides import overrides\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropTypeDatasetReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    DatasetReader\n",
    "    \"\"\"\n",
    "    def __init__(self, token_indexers: Dict[str, TokenIndexer] = None) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        \n",
    "    def text_to_instance(self, tokens: List[Token], tag: List[str] = None) -> Instance:\n",
    "        question_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"question\": question_field}\n",
    "\n",
    "        if tag:\n",
    "            label_field = LabelField(tag)\n",
    "            fields[\"label\"] = label_field\n",
    "        return Instance(fields)\n",
    "    \n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        with open(file_path) as json_file:  \n",
    "            data = json.load(json_file)\n",
    "            for k in data:\n",
    "                for elem in data[k]['qa_pairs']:\n",
    "                    question = elem['question']\n",
    "                    answer_type = \"\" \n",
    "                    for key in elem['answer']:\n",
    "                        if (key == 'number') and (len(elem['answer'][key]) != 0):\n",
    "                            answer_type = \"number\"\n",
    "                            break\n",
    "\n",
    "                        if (key == 'spans') and (len(elem['answer'][key]) != 0):\n",
    "                            answer_type = \"spans\"\n",
    "                            break\n",
    "\n",
    "                        answer_type = 'date'\n",
    "                    yield self.text_to_instance([Token(word) for word in question], answer_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = DropTypeDatasetReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77409it [00:14, 5301.04it/s]\n",
      "9536it [00:02, 3754.08it/s]\n"
     ]
    }
   ],
   "source": [
    "data_train = reader.read('../data/drop_dataset/drop_dataset_train.json')\n",
    "data_dev = reader.read('../data/drop_dataset/drop_dataset_dev.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 86945/86945 [00:05<00:00, 16222.83it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(data_train + data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model in AllenNLP represents a model that is trained.\n",
    "class LstmClassifier(Model):\n",
    "    def __init__(self,\n",
    "                 word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2VecEncoder,\n",
    "                 vocab: Vocabulary) -> None:\n",
    "        super().__init__(vocab)\n",
    "        # We need the embeddings to convert word IDs to their vector representations\n",
    "        self.word_embeddings = word_embeddings\n",
    "\n",
    "        # Seq2VecEncoder is a neural network abstraction that takes a sequence of something\n",
    "        # (usually a sequence of embedded word vectors), processes it, and returns it as a single\n",
    "        # vector. Oftentimes, this is an RNN-based architecture (e.g., LSTM or GRU), but\n",
    "        # AllenNLP also supports CNNs and other simple architectures (for example,\n",
    "        # just averaging over the input vectors).\n",
    "        self.encoder = encoder\n",
    "\n",
    "        # After converting a sequence of vectors to a single vector, we feed it into\n",
    "        # a fully-connected linear layer to reduce the dimension to the total number of labels.\n",
    "        self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                          out_features=vocab.get_vocab_size('labels'))\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "\n",
    "        # We use the cross-entropy loss because this is a classification task.\n",
    "        # Note that PyTorch's CrossEntropyLoss combines softmax and log likelihood loss,\n",
    "        # which makes it unnecessary to add a separate softmax layer.\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Instances are fed to forward after batching.\n",
    "    # Fields are passed through arguments with the same name.\n",
    "    def forward(self,\n",
    "                question: Dict[str, torch.Tensor],\n",
    "                label: torch.Tensor = None) -> torch.Tensor:\n",
    "        # In deep NLP, when sequences of tensors in different lengths are batched together,\n",
    "        # shorter sequences get padded with zeros to make them of equal length.\n",
    "        # Masking is the process to ignore extra zeros added by padding\n",
    "        mask = get_text_field_mask(question)\n",
    "\n",
    "        # Forward pass\n",
    "        embeddings = self.word_embeddings(question)\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        logits = self.hidden2tag(encoder_out)\n",
    "\n",
    "        # In AllenNLP, the output of forward() is a dictionary.\n",
    "        # Your output dictionary must contain a \"loss\" key for your model to be trained.\n",
    "        output = {\"logits\": logits}\n",
    "        if label is not None:\n",
    "            self.accuracy(logits, label)\n",
    "            output[\"loss\"] = self.loss_function(logits, label)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = PytorchSeq2VecWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
    "model = LstmClassifier(word_embeddings, lstm, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9634, loss: 0.1710 ||: 100%|██████████| 1210/1210 [00:38<00:00, 31.35it/s]\n",
      "accuracy: 0.9682, loss: 0.1512 ||: 100%|██████████| 149/149 [00:01<00:00, 82.29it/s]\n",
      "accuracy: 0.9641, loss: 0.1647 ||: 100%|██████████| 1210/1210 [00:39<00:00, 27.80it/s]\n",
      "accuracy: 0.9688, loss: 0.1463 ||: 100%|██████████| 149/149 [00:01<00:00, 84.66it/s]\n",
      "accuracy: 0.9622, loss: 0.1686 ||: 100%|██████████| 1210/1210 [00:38<00:00, 34.38it/s]\n",
      "accuracy: 0.9659, loss: 0.1533 ||: 100%|██████████| 149/149 [00:01<00:00, 86.49it/s]\n",
      "accuracy: 0.9636, loss: 0.1647 ||: 100%|██████████| 1210/1210 [00:40<00:00, 29.73it/s]\n",
      "accuracy: 0.9661, loss: 0.1644 ||: 100%|██████████| 149/149 [00:01<00:00, 87.19it/s]\n",
      "accuracy: 0.9653, loss: 0.1591 ||: 100%|██████████| 1210/1210 [00:38<00:00, 31.54it/s]\n",
      "accuracy: 0.9700, loss: 0.1403 ||: 100%|██████████| 149/149 [00:01<00:00, 86.95it/s]\n",
      "accuracy: 0.9651, loss: 0.1579 ||: 100%|██████████| 1210/1210 [00:43<00:00, 27.62it/s]\n",
      "accuracy: 0.9689, loss: 0.1427 ||: 100%|██████████| 149/149 [00:02<00:00, 54.29it/s]\n",
      "accuracy: 0.9661, loss: 0.1544 ||: 100%|██████████| 1210/1210 [00:44<00:00, 27.14it/s]\n",
      "accuracy: 0.9708, loss: 0.1364 ||: 100%|██████████| 149/149 [00:01<00:00, 88.42it/s]\n",
      "accuracy: 0.9654, loss: 0.1562 ||: 100%|██████████| 1210/1210 [00:37<00:00, 32.33it/s]\n",
      "accuracy: 0.9638, loss: 0.1619 ||: 100%|██████████| 149/149 [00:01<00:00, 88.78it/s]\n",
      "accuracy: 0.9658, loss: 0.1542 ||: 100%|██████████| 1210/1210 [00:37<00:00, 32.46it/s]\n",
      "accuracy: 0.9704, loss: 0.1361 ||: 100%|██████████| 149/149 [00:01<00:00, 89.59it/s]\n",
      "accuracy: 0.9658, loss: 0.1545 ||: 100%|██████████| 1210/1210 [00:37<00:00, 32.33it/s]\n",
      "accuracy: 0.9701, loss: 0.1370 ||: 100%|██████████| 149/149 [00:01<00:00, 89.04it/s]\n",
      "accuracy: 0.9666, loss: 0.1516 ||: 100%|██████████| 1210/1210 [00:37<00:00, 32.08it/s]\n",
      "accuracy: 0.9719, loss: 0.1304 ||: 100%|██████████| 149/149 [00:01<00:00, 89.39it/s]\n",
      "accuracy: 0.9669, loss: 0.1503 ||: 100%|██████████| 1210/1210 [00:37<00:00, 32.35it/s]\n",
      "accuracy: 0.9705, loss: 0.1355 ||: 100%|██████████| 149/149 [00:01<00:00, 84.55it/s]\n",
      "accuracy: 0.9665, loss: 0.1514 ||: 100%|██████████| 1210/1210 [00:39<00:00, 31.01it/s]\n",
      "accuracy: 0.9713, loss: 0.1329 ||: 100%|██████████| 149/149 [00:01<00:00, 61.74it/s]\n",
      "accuracy: 0.9679, loss: 0.1455 ||: 100%|██████████| 1210/1210 [00:40<00:00, 30.17it/s]\n",
      "accuracy: 0.9719, loss: 0.1301 ||: 100%|██████████| 149/149 [00:01<00:00, 87.26it/s]\n",
      "accuracy: 0.9682, loss: 0.1443 ||: 100%|██████████| 1210/1210 [00:38<00:00, 31.17it/s]\n",
      "accuracy: 0.9723, loss: 0.1283 ||: 100%|██████████| 149/149 [00:01<00:00, 80.71it/s]\n",
      "accuracy: 0.9685, loss: 0.1431 ||: 100%|██████████| 1210/1210 [00:37<00:00, 32.05it/s]\n",
      "accuracy: 0.9722, loss: 0.1278 ||: 100%|██████████| 149/149 [00:01<00:00, 89.34it/s]\n",
      "accuracy: 0.9665, loss: 0.1513 ||: 100%|██████████| 1210/1210 [00:39<00:00, 30.43it/s]\n",
      "accuracy: 0.9680, loss: 0.1465 ||: 100%|██████████| 149/149 [00:01<00:00, 83.95it/s]\n",
      "accuracy: 0.9646, loss: 0.1589 ||: 100%|██████████| 1210/1210 [00:38<00:00, 31.08it/s]\n",
      "accuracy: 0.9692, loss: 0.1405 ||: 100%|██████████| 149/149 [00:01<00:00, 82.76it/s]\n",
      "accuracy: 0.9651, loss: 0.1564 ||: 100%|██████████| 1210/1210 [00:40<00:00, 29.68it/s]\n",
      "accuracy: 0.9700, loss: 0.1361 ||: 100%|██████████| 149/149 [00:01<00:00, 81.45it/s]\n",
      "accuracy: 0.9664, loss: 0.1508 ||: 100%|██████████| 1210/1210 [00:39<00:00, 30.64it/s]\n",
      "accuracy: 0.9715, loss: 0.1302 ||: 100%|██████████| 149/149 [00:01<00:00, 85.70it/s]\n",
      "accuracy: 0.9670, loss: 0.1480 ||: 100%|██████████| 1210/1210 [00:41<00:00, 34.88it/s]\n",
      "accuracy: 0.9714, loss: 0.1299 ||: 100%|██████████| 149/149 [00:01<00:00, 88.75it/s]\n",
      "accuracy: 0.9683, loss: 0.1433 ||: 100%|██████████| 1210/1210 [00:39<00:00, 33.48it/s]\n",
      "accuracy: 0.9733, loss: 0.1246 ||: 100%|██████████| 149/149 [00:01<00:00, 84.51it/s]\n",
      "accuracy: 0.9690, loss: 0.1407 ||: 100%|██████████| 1210/1210 [00:40<00:00, 30.12it/s]\n",
      "accuracy: 0.9728, loss: 0.1255 ||: 100%|██████████| 149/149 [00:01<00:00, 84.26it/s]\n",
      "accuracy: 0.9689, loss: 0.1403 ||: 100%|██████████| 1210/1210 [00:39<00:00, 30.27it/s]\n",
      "accuracy: 0.9725, loss: 0.1261 ||: 100%|██████████| 149/149 [00:01<00:00, 86.93it/s]\n",
      "accuracy: 0.9686, loss: 0.1413 ||: 100%|██████████| 1210/1210 [00:38<00:00, 31.51it/s]\n",
      "accuracy: 0.9714, loss: 0.1311 ||: 100%|██████████| 149/149 [00:01<00:00, 82.83it/s]\n",
      "accuracy: 0.9689, loss: 0.1398 ||: 100%|██████████| 1210/1210 [00:40<00:00, 29.84it/s]\n",
      "accuracy: 0.9734, loss: 0.1233 ||: 100%|██████████| 149/149 [00:01<00:00, 77.73it/s]\n",
      "accuracy: 0.9694, loss: 0.1376 ||: 100%|██████████| 1210/1210 [00:39<00:00, 30.48it/s]\n",
      "accuracy: 0.9729, loss: 0.1240 ||: 100%|██████████| 149/149 [00:01<00:00, 62.45it/s]\n",
      "accuracy: 0.9691, loss: 0.1389 ||: 100%|██████████| 1210/1210 [00:42<00:00, 28.79it/s]\n",
      "accuracy: 0.9728, loss: 0.1251 ||: 100%|██████████| 149/149 [00:01<00:00, 59.78it/s]\n",
      "accuracy: 0.9693, loss: 0.1381 ||: 100%|██████████| 1210/1210 [00:38<00:00, 31.27it/s]\n",
      "accuracy: 0.9736, loss: 0.1201 ||: 100%|██████████| 149/149 [00:01<00:00, 85.63it/s]\n",
      "accuracy: 0.9689, loss: 0.1399 ||: 100%|██████████| 1210/1210 [00:39<00:00, 30.66it/s]\n",
      "accuracy: 0.9717, loss: 0.1291 ||: 100%|██████████| 149/149 [00:01<00:00, 80.51it/s]\n",
      "accuracy: 0.9697, loss: 0.1358 ||: 100%|██████████| 1210/1210 [00:40<00:00, 29.94it/s]\n",
      "accuracy: 0.9699, loss: 0.1377 ||: 100%|██████████| 149/149 [00:01<00:00, 82.76it/s]\n",
      "accuracy: 0.9693, loss: 0.1376 ||: 100%|██████████| 1210/1210 [00:39<00:00, 30.35it/s]\n",
      "accuracy: 0.9742, loss: 0.1179 ||: 100%|██████████| 149/149 [00:01<00:00, 62.13it/s]\n",
      "accuracy: 0.9704, loss: 0.1322 ||: 100%|██████████| 1210/1210 [00:41<00:00, 30.01it/s]\n",
      "accuracy: 0.9739, loss: 0.1187 ||: 100%|██████████| 149/149 [00:01<00:00, 90.21it/s]\n",
      "accuracy: 0.9692, loss: 0.1386 ||: 100%|██████████| 1210/1210 [00:37<00:00, 32.10it/s]\n",
      "accuracy: 0.9732, loss: 0.1221 ||: 100%|██████████| 149/149 [00:01<00:00, 88.90it/s]\n",
      "accuracy: 0.9698, loss: 0.1355 ||: 100%|██████████| 1210/1210 [00:37<00:00, 33.28it/s]\n",
      "accuracy: 0.9736, loss: 0.1201 ||: 100%|██████████| 149/149 [00:01<00:00, 88.78it/s]\n",
      "accuracy: 0.9701, loss: 0.1335 ||: 100%|██████████| 1210/1210 [00:37<00:00, 32.49it/s]\n",
      "accuracy: 0.9737, loss: 0.1191 ||: 100%|██████████| 149/149 [00:01<00:00, 87.50it/s]\n",
      "accuracy: 0.9705, loss: 0.1320 ||: 100%|██████████| 1210/1210 [00:37<00:00, 32.49it/s]\n",
      "accuracy: 0.9736, loss: 0.1200 ||: 100%|██████████| 149/149 [00:01<00:00, 87.86it/s]\n",
      "accuracy: 0.9703, loss: 0.1329 ||: 100%|██████████| 1210/1210 [00:37<00:00, 32.35it/s]\n",
      "accuracy: 0.9749, loss: 0.1140 ||: 100%|██████████| 149/149 [00:01<00:00, 88.32it/s]\n",
      "accuracy: 0.9708, loss: 0.1302 ||: 100%|██████████| 1210/1210 [00:40<00:00, 29.57it/s]\n",
      "accuracy: 0.9747, loss: 0.1140 ||: 100%|██████████| 149/149 [00:01<00:00, 88.78it/s]\n",
      "accuracy: 0.9703, loss: 0.1323 ||: 100%|██████████| 1210/1210 [00:37<00:00, 31.89it/s]\n",
      "accuracy: 0.9737, loss: 0.1191 ||: 100%|██████████| 149/149 [00:01<00:00, 88.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 37,\n",
       " 'peak_cpu_memory_MB': 1307.824128,\n",
       " 'training_duration': '00:27:24',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 39,\n",
       " 'epoch': 39,\n",
       " 'training_accuracy': 0.9703006110400599,\n",
       " 'training_loss': 0.1322935859033884,\n",
       " 'training_cpu_memory_MB': 1307.824128,\n",
       " 'validation_accuracy': 0.9736786912751678,\n",
       " 'validation_loss': 0.11910912504532194,\n",
       " 'best_validation_accuracy': 0.9749370805369127,\n",
       " 'best_validation_loss': 0.11396335590405753}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "iterator = BucketIterator(batch_size=64, sorting_keys=[(\"question\", \"num_tokens\")])\n",
    "iterator.index_with(vocab)\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=data_train,\n",
    "                  validation_dataset=data_dev,\n",
    "                  patience=10,\n",
    "                  num_epochs=40,\n",
    "                  cuda_device=-1)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConfigurationError",
     "evalue": "'Cannot register sentence_classifier_predictor2 as Predictor; name already in use for SentenceClassifierPredictor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConfigurationError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-31a946345c61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Note that you need to use \"@Predictor.register\", not \"@Model.register\"!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mPredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sentence_classifier_predictor2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSentenceClassifierPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPredictor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_reader\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDatasetReader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_reader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Stanford/winter2019/pyEnv/lib/python3.7/site-packages/allennlp/common/registrable.py\u001b[0m in \u001b[0;36madd_subclass_to_registry\u001b[0;34m(subclass)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 message = \"Cannot register %s as %s; name already in use for %s\" % (\n\u001b[1;32m     48\u001b[0m                         name, cls.__name__, registry[name].__name__)\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mConfigurationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mregistry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msubclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConfigurationError\u001b[0m: 'Cannot register sentence_classifier_predictor2 as Predictor; name already in use for SentenceClassifierPredictor'"
     ]
    }
   ],
   "source": [
    "# You need to name your predictor and register so that `allennlp` command can recognize it\n",
    "# Note that you need to use \"@Predictor.register\", not \"@Model.register\"!\n",
    "@Predictor.register(\"sentence_classifier_predictor2\")\n",
    "class SentenceClassifierPredictor(Predictor):\n",
    "    def __init__(self, model: Model, dataset_reader: DatasetReader) -> None:\n",
    "        super().__init__(model, dataset_reader)\n",
    "        self._tokenizer = SpacyWordSplitter(language='en_core_web_sm', pos_tags=True)\n",
    "\n",
    "    def predict(self, sentence: str) -> JsonDict:\n",
    "        return self.predict_json({\"question\" : question})\n",
    "\n",
    "    @overrides\n",
    "    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n",
    "        sentence = json_dict[\"question\"]\n",
    "        tokens = self._tokenizer.split_words(question)\n",
    "        return self._dataset_reader.text_to_instance([str(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument 'string' has incorrect type (expected str, got list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c98a595f5e1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'This'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'the'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'best'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'movie'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ever'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'!'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSentenceClassifierPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_reader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlabel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-00fb9c34fba8>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"sentence\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Stanford/winter2019/pyEnv/lib/python3.7/site-packages/allennlp/predictors/predictor.py\u001b[0m in \u001b[0;36mpredict_json\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json_to_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-00fb9c34fba8>\u001b[0m in \u001b[0;36m_json_to_instance\u001b[0;34m(self, json_dict)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_json_to_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_dict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mJsonDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mInstance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sentence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_to_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Stanford/winter2019/pyEnv/lib/python3.7/site-packages/allennlp/data/tokenizers/word_splitter.py\u001b[0m in \u001b[0;36msplit_words\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msplit_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mToken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;31m# This works because our Token class matches spacy's.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_remove_spaces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Stanford/winter2019/pyEnv/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             )\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Stanford/winter2019/pyEnv/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument 'string' has incorrect type (expected str, got list)"
     ]
    }
   ],
   "source": [
    "tokens = ['This', 'is', 'the', 'best', 'movie', 'ever', '!']\n",
    "predictor = SentenceClassifierPredictor(model, dataset_reader=reader)\n",
    "logits = predictor.predict(tokens)['logits']\n",
    "label_id = np.argmax(logits)\n",
    "\n",
    "print(model.vocab.get_token_from_index(label_id, 'labels'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
